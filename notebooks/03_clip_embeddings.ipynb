{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_clip_embeddings.ipynb\n",
    "## Pruebas de CLIP Embeddings y Espacio Vectorial Compartido\n",
    "\n",
    "**CR√çTICO**: Valida que imagen y texto est√©n en el MISMO espacio vectorial\n",
    "\n",
    "Esto es el CORAZ√ìN del \"Enfoque 3: RAG Multimodal Verdadero\" de Clase 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.embeddings import CLIPEncoder\n",
    "from src.utils.config import EXCEL_IMAGES_DIR, PDF_IMAGES_DIR\n",
    "from src.utils.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicializar CLIP Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cargando modelo CLIP pre-entrenado...\")\nencoder = CLIPEncoder()\nprint(\"‚úÖ CLIP encoder listo\")\n\n# Informaci√≥n del modelo\nprint(f\"\\nInformaci√≥n del modelo:\")\nprint(f\"  - Modelo: openai/clip-vit-base-patch32\")\nprint(f\"  - Dimensionalidad: 512\")\nprint(f\"  - Pre-entrenado en: ~400M pares imagen-texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Codificar Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener imagen de prueba\nexcel_images = list(EXCEL_IMAGES_DIR.glob(\"*.png\"))\n\nif excel_images:\n    test_image = excel_images[0]\n    print(f\"Codificando imagen: {test_image.name}\")\n    \n    # Encodear imagen\n    image_embedding = encoder.encode_image(str(test_image))\n    \n    if image_embedding is not None:\n        print(f\"\\n‚úÖ Embedding generado:\")\n        print(f\"   - Dimensiones: {image_embedding.shape}\")\n        print(f\"   - Tipo: {type(image_embedding)}\")\n        print(f\"   - Primeros 5 valores: {image_embedding[:5]}\")\n        print(f\"   - Norma (debe ser ~1.0): {np.linalg.norm(image_embedding):.4f}\")\n    else:\n        print(\"‚ùå Error al codificar imagen\")\nelse:\n    print(\"‚ö†Ô∏è No hay im√°genes. Ejecuta notebook 01.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Codificar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textos de prueba\ntest_texts = [\n    \"¬øCu√°l es el total de la liquidaci√≥n?\",\n    \"Liquidaci√≥n de sueldo y gratificaciones\",\n    \"Tabla con montos y conceptos de pago\",\n    \"Esto es un documento completamente diferente sobre gatos\"\n]\n\ntext_embeddings = {}\n\nfor text in test_texts:\n    print(f\"\\nCodificando: '{text[:50]}...'\")\n    embedding = encoder.encode_text(text)\n    \n    if embedding is not None:\n        text_embeddings[text] = embedding\n        print(f\"  ‚úÖ Dimensiones: {embedding.shape}\")\n        print(f\"  ‚úÖ Norma: {np.linalg.norm(embedding):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ‚≠ê VERIFICAR ESPACIO COMPARTIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTO ES CR√çTICO: Verificar que imagen y texto pueden compararse\n\nprint(\"=\"*60)\nprint(\"VERIFICACI√ìN ESPACIO VECTORIAL COMPARTIDO\")\nprint(\"=\"*60)\n\nif 'image_embedding' in locals() and text_embeddings:\n    # Verificar dimensiones\n    img_dim = image_embedding.shape[0]\n    text_dim = text_embeddings[test_texts[0]].shape[0]\n    \n    print(f\"\\n1. Dimensionalidad:\")\n    print(f\"   Imagen: {img_dim}\")\n    print(f\"   Texto: {text_dim}\")\n    print(f\"   ¬øIguales? {'‚úÖ S√ç' if img_dim == text_dim else '‚ùå NO'}\")\n    \n    # Calcular similitud coseno\n    print(f\"\\n2. Similitud Coseno (Imagen vs Textos):\")\n    print(f\"   {'Texto':<40} | {'Similitud':>10}\")\n    print(f\"   {'-'*40}-{'-'*10}\")\n    \n    for text, embedding in text_embeddings.items():\n        similarity = np.dot(image_embedding, embedding)\n        print(f\"   {text[:39]:<40} | {similarity:>10.4f}\")\n    \n    print(f\"\\n3. Interpretaci√≥n:\")\n    print(f\"   - Valores cerca de 1.0 = SIMILARES\")\n    print(f\"   - Valores cerca de 0.0 = DIFERENTES\")\n    print(f\"   - Valores cerca de -1.0 = OPUESTOS\")\n    \n    print(f\"\\n‚úÖ CONCLUSI√ìN: Imagen y texto est√°n en MISMO espacio\\n\")\nelse:\n    print(\"‚ùå Faltan embeddings para comparaci√≥n\")\n\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. M√©todo oficial: verify_shared_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar m√©todo oficial del encoder\nif excel_images:\n    test_image = excel_images[0]\n    test_text = test_texts[0]\n    \n    print(f\"Verificando espacio compartido...\\n\")\n    verification = encoder.verify_shared_space(str(test_image), test_text)\n    \n    print(f\"Resultado verificaci√≥n:\")\n    print(json.dumps(verification, indent=2))\n    \n    if verification.get('shared_space'):\n        print(f\"\\n‚úÖ √âXITO: Espacio vectorial compartido verificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing: M√∫ltiples Im√°genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar m√∫ltiples im√°genes\nall_images = list(EXCEL_IMAGES_DIR.glob(\"*.png\")) + list(PDF_IMAGES_DIR.glob(\"*.png\"))\nimage_paths = [str(img) for img in all_images[:5]]  # Primeras 5\n\nif image_paths:\n    print(f\"Procesando {len(image_paths)} im√°genes en batch...\\n\")\n    \n    embeddings = encoder.batch_encode_images(image_paths)\n    \n    print(f\"\\nResultados:\")\n    print(f\"  Im√°genes procesadas: {len(embeddings)}\")\n    \n    for path, data in list(embeddings.items())[:3]:\n        print(f\"\\n  - {Path(path).name}\")\n        print(f\"    Dimensi√≥n: {np.array(data['embedding']).shape}\")\n        print(f\"    Tipo: {data['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing: M√∫ltiples Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar m√∫ltiples textos\ntest_docs = [\n    \"Liquidaci√≥n de abril de 2024 con bonificaci√≥n\",\n    \"Pago de sueldo base y AFP\",\n    \"Resumen de ingresos y descuentos\",\n    \"Tabla de conceptos de pago con montos\",\n    \"Documento de recursos humanos\"\n]\n\nprint(f\"Procesando {len(test_docs)} textos en batch...\\n\")\n\ntext_embeddings_batch = encoder.batch_encode_texts(test_docs)\n\nprint(f\"\\nResultados:\")\nprint(f\"  Textos procesados: {len(text_embeddings_batch)}\")\n\nfor key, data in list(text_embeddings_batch.items())[:3]:\n    print(f\"\\n  - {key}\")\n    print(f\"    Contenido: {data['content']}\")\n    print(f\"    Dimensi√≥n: {np.array(data['embedding']).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizaci√≥n: t-SNE del Espacio Vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el espacio vectorial con t-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nif embeddings and text_embeddings_batch:\n",
    "    print(\"Reduciendo dimensionalidad con t-SNE...\")\n",
    "    \n",
    "    # Recolectar todos los embeddings\n",
    "    all_embeddings = []\n",
    "    labels = []\n",
    "    colors_list = []\n",
    "    \n",
    "    # Im√°genes\n",
    "    for path, data in list(embeddings.items())[:3]:\n",
    "        all_embeddings.append(data['embedding'])\n",
    "        labels.append(Path(path).name[:20])\n",
    "        colors_list.append('red')\n",
    "    \n",
    "    # Textos\n",
    "    for key, data in list(text_embeddings_batch.items())[:3]:\n",
    "        all_embeddings.append(data['embedding'])\n",
    "        labels.append(f\"Text: {data['content'][:15]}\")\n",
    "        colors_list.append('blue')\n",
    "    \n",
    "    # Aplicar t-SNE\n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(all_embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Plotear\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, (x, y) in enumerate(embeddings_2d):\n",
    "        plt.scatter(x, y, s=200, c=colors_list[i], alpha=0.7, edgecolors='black')\n",
    "        plt.annotate(labels[i], (x, y), fontsize=8, ha='center')\n",
    "    \n",
    "    plt.title('Espacio Vectorial CLIP (t-SNE)\\nRojo: Im√°genes | Azul: Textos', fontsize=14)\n",
    "    plt.xlabel('Dimensi√≥n 1 (t-SNE)')\n",
    "    plt.ylabel('Dimensi√≥n 2 (t-SNE)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig('clip_embeddings_tsne.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizaci√≥n guardada: clip_embeddings_tsne.png\")\nelse:\n",
    "    print(\"‚ùå No hay suficientes embeddings para visualizar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Matriz de Similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de similitud imagen-texto\nimport seaborn as sns\n\nif embeddings and text_embeddings_batch:\n",
    "    print(\"Calculando matriz de similitud...\\n\")\n",
    "    \n",
    "    image_embs = [data['embedding'] for data in list(embeddings.values())[:3]]\n",
    "    text_embs = [data['embedding'] for data in list(text_embeddings_batch.values())[:3]]\n",
    "    image_labels = [Path(path).name[:15] for path in list(embeddings.keys())[:3]]\n",
    "    text_labels = [data['content'][:15] for data in list(text_embeddings_batch.values())[:3]]\n",
    "    \n",
    "    # Calcular similitudes\n",
    "    similarity_matrix = np.zeros((len(image_embs), len(text_embs)))\n",
    "    \n",
    "    for i, img_emb in enumerate(image_embs):\n",
    "        for j, text_emb in enumerate(text_embs):\n",
    "            similarity = np.dot(img_emb, text_emb)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    # Plotear\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt='.3f', \n",
    "                xticklabels=text_labels, yticklabels=image_labels,\n",
    "                cmap='RdYlGn', center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Matriz de Similitud: Im√°genes vs Textos\\n(Espacio Compartido CLIP)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('clip_similarity_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Matriz guardada: clip_similarity_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Embeddings para Indexaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar embeddings en formato JSON\nfrom src.embeddings import save_embeddings\n\nif embeddings:\n",
    "    print(\"Guardando embeddings de im√°genes...\")\n",
    "    success = save_embeddings(embeddings, 'image_embeddings_test.json')\n",
    "    if success:\n",
    "        print(\"‚úÖ Embeddings guardados\")\n\nif text_embeddings_batch:\n",
    "    print(\"\\nGuardando embeddings de textos...\")\n",
    "    success = save_embeddings(text_embeddings_batch, 'text_embeddings_test.json')\n",
    "    if success:\n",
    "        print(\"‚úÖ Embeddings guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumen y Conceptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\nprint(\"RESUMEN: CLIP EMBEDDINGS & ESPACIO MULTIMODAL\")\nprint(\"=\"*70)\n\nprint(f\"\\n‚úÖ LOGROS:\")\nprint(f\"   1. Cargar modelo CLIP pre-entrenado\")\nprint(f\"   2. Codificar im√°genes ‚Üí 512 dimensiones\")\nprint(f\"   3. Codificar textos ‚Üí 512 dimensiones (MISMO espacio)\")\nprint(f\"   4. Calcular similitud entre imagen ‚Üî texto\")\nprint(f\"   5. Visualizar con t-SNE\")\nprint(f\"   6. Crear matriz de similitud\")\nprint(f\"   7. Guardar embeddings en JSON\")\n\nprint(f\"\\nüìö CONCEPTOS CLASE 17:\")\nprint(f\"   ‚úÖ Multimodalidad: Imagen + Texto en mismo espacio\")\nprint(f\"   ‚úÖ CLIP: Modelo pre-entrenado OpenAI\")\nprint(f\"   ‚úÖ Espacio Compartido: 512 dimensiones para ambos tipos\")\nprint(f\"   ‚úÖ Similitud Coseno: Comparar imagen ‚Üî texto\")\nprint(f\"   ‚úÖ RAG Multimodal: Base para retrieval\")\n\nprint(f\"\\nüöÄ PR√ìXIMO PASO:\")\nprint(f\"   ‚Üí Notebook 04: Agent Prototype (LangGraph + ChromaDB)\")\nprint(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
